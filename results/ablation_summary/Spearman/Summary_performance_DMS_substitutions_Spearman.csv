Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Average_Spearman_fold_random_5,Average_Spearman_fold_modulo_5,Average_Spearman_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,Kermut (EVE),,0.662,0.006,0.75,0.627,0.608,0.604,0.643,0.661,0.58,0.821,0.731,0.625,0.688,0.72,0.679,0.696,0.7,,
2,Kermut,Embedding,0.659,0.0,0.743,0.628,0.605,0.602,0.625,0.665,0.578,0.824,0.745,0.606,0.681,0.722,0.68,0.709,0.645,"<a href='https://www.biorxiv.org/content/10.1101/2024.05.28.596219v1'>Groth, P. M., Kerrn, M. H., Olsen, L., Salomon, J., Boomsma, W. (2024).  Kermut: Composite kernel regression for protein variant effects</a>",Kermut GP
3,Kermut (VESPA),,0.657,0.005,0.742,0.623,0.606,0.601,0.62,0.66,0.585,0.82,0.737,0.61,0.686,0.714,0.678,0.705,0.678,,
4,Kermut (GEMME),,0.657,0.005,0.744,0.622,0.605,0.598,0.623,0.663,0.579,0.822,0.733,0.619,0.682,0.716,0.675,0.697,0.695,,
5,Kermut (TranceptEVE),,0.654,0.004,0.744,0.619,0.6,0.596,0.619,0.662,0.573,0.821,0.739,0.605,0.678,0.712,0.672,0.703,0.668,,
6,Kermut (no Hellinger),,0.653,0.002,0.743,0.619,0.599,0.59,0.619,0.664,0.574,0.82,0.742,0.593,0.676,0.71,0.678,0.705,0.631,,
7,Kermut (no p),,0.643,0.004,0.738,0.609,0.583,0.59,0.611,0.651,0.564,0.801,0.728,0.591,0.66,0.701,0.669,0.689,0.611,,
8,Kermut (ESM-IF1),,0.642,0.007,0.738,0.606,0.583,0.588,0.585,0.654,0.551,0.832,0.737,0.581,0.676,0.709,0.666,0.7,0.658,,
9,Kermut (ProteinMPNN),,0.636,0.006,0.734,0.599,0.575,0.582,0.583,0.651,0.54,0.823,0.728,0.578,0.666,0.699,0.66,0.689,0.649,,
10,Kermut (const. mean),,0.633,0.007,0.735,0.596,0.569,0.579,0.58,0.651,0.536,0.821,0.723,0.577,0.664,0.697,0.658,0.683,0.648,,
11,Kermut (no Hellinger/p),,0.633,0.006,0.737,0.591,0.57,0.569,0.601,0.648,0.557,0.79,0.72,0.566,0.649,0.679,0.663,0.68,0.59,,
12,Kermut (no distance),,0.624,0.004,0.739,0.577,0.556,0.562,0.56,0.634,0.546,0.818,0.734,0.562,0.654,0.703,0.658,0.686,0.602,,
13,Kermut (no global),,0.614,0.006,0.695,0.582,0.565,0.578,0.616,0.611,0.547,0.716,0.672,0.571,0.621,0.662,0.616,0.641,0.606,,
14,ProteinNPT,Embedding,0.611,0.01,0.74,0.554,0.54,0.574,0.519,0.637,0.554,0.772,0.707,0.568,0.627,0.666,0.643,0.667,0.605,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
15,Kermut (no m),,0.594,0.009,0.71,0.55,0.523,0.531,0.529,0.614,0.519,0.778,0.703,0.51,0.619,0.654,0.638,0.654,0.52,,
16,MSA Transformer Embeddings,Embedding,0.585,0.015,0.672,0.549,0.534,0.559,0.459,0.622,0.535,0.749,0.694,0.536,0.602,0.653,0.619,0.648,0.573,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
17,Tranception Embeddings,Embedding,0.571,0.009,0.709,0.52,0.485,0.518,0.516,0.625,0.525,0.674,0.626,0.544,0.577,0.585,0.59,0.593,0.604,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
18,ESM-1v Embeddings,Embedding,0.551,0.013,0.663,0.509,0.482,0.48,0.434,0.613,0.514,0.717,0.661,0.467,0.583,0.585,0.596,0.615,0.537,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
19,"Kermut (no m, const. mean)",,0.547,0.011,0.694,0.49,0.456,0.471,0.454,0.589,0.457,0.761,0.665,0.436,0.585,0.591,0.598,0.613,0.518,,
20,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.477,0.015,0.559,0.435,0.435,0.506,0.434,0.464,0.487,0.493,0.506,0.483,0.473,0.493,0.497,0.474,0.483,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
21,Tranception + One-Hot Encodings,One-hot Encoding,0.458,0.014,0.545,0.414,0.414,0.482,0.396,0.469,0.469,0.473,0.492,0.453,0.452,0.468,0.48,0.453,0.467,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
22,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.455,0.017,0.551,0.408,0.405,0.481,0.374,0.464,0.464,0.491,0.502,0.452,0.454,0.493,0.474,0.465,0.458,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
23,DeepSequence + One-Hot Encodings,One-hot Encoding,0.439,0.02,0.532,0.393,0.391,0.467,0.406,0.404,0.447,0.471,0.484,0.429,0.436,0.462,0.465,0.454,0.406,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
24,ESM-1v + One-Hot Encodings,One-hot Encoding,0.417,0.016,0.53,0.362,0.359,0.403,0.335,0.446,0.438,0.463,0.497,0.343,0.421,0.437,0.446,0.447,0.373,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
25,One-Hot Encodings,One-hot Encoding,0.227,0.017,0.577,0.033,0.072,0.222,0.207,0.233,0.201,0.273,0.247,0.205,0.239,0.247,0.225,0.231,0.274,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
