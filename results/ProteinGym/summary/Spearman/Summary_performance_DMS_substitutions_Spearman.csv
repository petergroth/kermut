Model_rank,Model_name,Model type,Average_Spearman,Average_Spearman_fold_random_5,Average_Spearman_fold_modulo_5,Average_Spearman_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,MSA Transformer Embeddings,Embedding,0.598,0.685,0.565,0.545,0.541,0.53,0.62,0.544,0.755,0.701,0.575,0.637,0.73,0.647,0.669,0.578,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
2,ProteinNPT,Embedding,0.594,0.704,0.544,0.536,0.539,0.471,0.636,0.55,0.775,0.716,0.519,0.649,0.732,0.654,0.688,0.59,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
3,kermut_ProteinMPNN_TranceptEVE_MSAT,,0.58,0.71,0.539,0.49,0.514,0.471,0.623,0.546,0.743,0.687,0.531,0.63,0.725,0.632,0.654,0.602,,
4,Tranception Embeddings,Embedding,0.555,0.685,0.5,0.481,0.492,0.483,0.613,0.513,0.676,0.629,0.527,0.589,0.649,0.594,0.595,0.603,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
5,ESM-1v Embeddings,Embedding,0.521,0.637,0.473,0.454,0.476,0.292,0.61,0.51,0.718,0.666,0.372,0.602,0.674,0.602,0.624,0.576,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
6,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.48,0.53,0.453,0.457,0.49,0.439,0.483,0.489,0.498,0.509,0.529,0.473,0.504,0.509,0.47,0.47,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
7,DeepSequence + One-Hot Encodings,One-hot Encoding,0.464,0.51,0.441,0.44,0.451,0.49,0.431,0.473,0.474,0.491,0.495,0.443,0.472,0.482,0.461,0.402,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
8,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.462,0.525,0.432,0.429,0.465,0.392,0.488,0.468,0.495,0.502,0.514,0.462,0.503,0.493,0.466,0.45,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
9,Tranception + One-Hot Encodings,One-hot Encoding,0.458,0.51,0.429,0.433,0.474,0.396,0.477,0.462,0.479,0.492,0.503,0.449,0.487,0.49,0.44,0.472,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
10,ESM-1v + One-Hot Encodings,One-hot Encoding,0.421,0.52,0.374,0.37,0.404,0.312,0.469,0.455,0.466,0.498,0.406,0.412,0.474,0.459,0.44,0.414,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
11,One-Hot Encodings,One-hot Encoding,0.225,0.52,0.052,0.102,0.218,0.164,0.245,0.223,0.273,0.26,0.258,0.238,0.26,0.242,0.246,0.268,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
