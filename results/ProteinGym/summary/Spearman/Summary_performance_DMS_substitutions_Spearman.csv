Model_rank,Model_name,Model type,Average_Spearman,Average_Spearman_fold_random_5,Average_Spearman_fold_modulo_5,Average_Spearman_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,ProteinNPT,Embedding,0.609,0.729,0.556,0.543,0.564,0.517,0.633,0.558,0.774,0.717,0.553,0.642,0.7,0.643,0.688,0.59,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
2,MSA Transformer Embeddings,Embedding,0.584,0.662,0.553,0.538,0.551,0.461,0.622,0.534,0.754,0.701,0.534,0.618,0.688,0.623,0.667,0.556,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
3,Tranception Embeddings,Embedding,0.566,0.702,0.514,0.483,0.529,0.486,0.62,0.524,0.674,0.629,0.547,0.588,0.627,0.587,0.6,0.599,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
4,kermut_ProteinMPNN_TranceptEVE,,0.552,0.703,0.501,0.452,0.502,0.465,0.569,0.519,0.705,0.652,0.51,0.587,0.647,0.582,0.617,0.579,,
5,ESM-1v Embeddings,Embedding,0.539,0.649,0.496,0.472,0.507,0.345,0.615,0.509,0.718,0.666,0.468,0.587,0.648,0.592,0.62,0.529,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
6,kermutBH_oh_ESM_IF1,,0.534,0.699,0.473,0.428,0.458,0.425,0.551,0.477,0.756,0.672,0.457,0.587,0.661,0.569,0.641,0.57,,
7,kermutBH_oh,,0.491,0.681,0.428,0.364,0.419,0.41,0.53,0.435,0.661,0.596,0.441,0.522,0.577,0.52,0.566,0.508,,
8,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.477,0.551,0.44,0.44,0.512,0.417,0.484,0.481,0.492,0.507,0.471,0.479,0.505,0.492,0.48,0.47,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
9,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.463,0.541,0.425,0.422,0.478,0.402,0.487,0.454,0.492,0.502,0.454,0.462,0.502,0.477,0.473,0.436,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
10,Tranception + One-Hot Encodings,One-hot Encoding,0.456,0.535,0.417,0.417,0.495,0.374,0.483,0.457,0.473,0.49,0.45,0.456,0.485,0.476,0.452,0.452,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
11,DeepSequence + One-Hot Encodings,One-hot Encoding,0.443,0.522,0.406,0.403,0.475,0.399,0.422,0.451,0.471,0.489,0.421,0.445,0.478,0.46,0.47,0.389,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
12,ESM-1v + One-Hot Encodings,One-hot Encoding,0.418,0.523,0.367,0.364,0.438,0.282,0.474,0.431,0.465,0.497,0.383,0.416,0.482,0.445,0.446,0.36,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
13,One-Hot Encodings,One-hot Encoding,0.229,0.562,0.042,0.082,0.22,0.206,0.233,0.212,0.273,0.26,0.181,0.243,0.256,0.224,0.248,0.275,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
