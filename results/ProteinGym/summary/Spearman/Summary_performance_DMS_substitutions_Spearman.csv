Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Average_Spearman_fold_random_5,Average_Spearman_fold_modulo_5,Average_Spearman_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,ProteinNPT,Embedding,0.607,0.0,0.726,0.555,0.54,0.574,0.514,0.632,0.542,0.772,0.701,0.58,0.61,0.648,0.625,0.668,0.583,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
2,Tranception Embeddings,Embedding,0.571,0.008,0.696,0.526,0.49,0.52,0.529,0.613,0.519,0.674,0.621,0.556,0.561,0.569,0.582,0.594,0.568,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
3,MSA Transformer Embeddings,Embedding,0.564,0.009,0.64,0.532,0.52,0.545,0.452,0.582,0.492,0.749,0.685,0.512,0.57,0.634,0.578,0.648,0.524,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
4,ESM-1v Embeddings,Embedding,0.538,0.013,0.637,0.5,0.476,0.485,0.429,0.584,0.475,0.717,0.653,0.46,0.548,0.564,0.577,0.617,0.449,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
5,kermutBH_oh_ESM_IF1,,0.534,0.014,0.699,0.473,0.428,0.458,0.425,0.551,0.477,0.756,0.672,0.457,0.587,0.661,0.569,0.641,0.57,,
6,kermutBH_oh,,0.491,0.015,0.681,0.428,0.364,0.419,0.41,0.53,0.435,0.661,0.596,0.441,0.522,0.577,0.52,0.566,0.508,,
7,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.471,0.012,0.545,0.433,0.434,0.5,0.424,0.465,0.471,0.493,0.5,0.484,0.467,0.479,0.489,0.475,0.478,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
8,Tranception + One-Hot Encodings,One-hot Encoding,0.452,0.012,0.531,0.412,0.412,0.474,0.395,0.466,0.45,0.473,0.487,0.455,0.445,0.455,0.471,0.453,0.458,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
9,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.447,0.012,0.533,0.405,0.403,0.477,0.373,0.455,0.437,0.491,0.499,0.442,0.447,0.48,0.459,0.468,0.444,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
10,DeepSequence + One-Hot Encodings,One-hot Encoding,0.434,0.015,0.517,0.393,0.392,0.466,0.396,0.411,0.425,0.471,0.48,0.421,0.428,0.45,0.459,0.455,0.383,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
11,ESM-1v + One-Hot Encodings,One-hot Encoding,0.412,0.015,0.512,0.362,0.361,0.418,0.337,0.443,0.396,0.463,0.494,0.34,0.407,0.423,0.442,0.452,0.316,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
12,One-Hot Encodings,One-hot Encoding,0.222,0.015,0.576,0.026,0.064,0.216,0.201,0.227,0.192,0.273,0.245,0.202,0.228,0.239,0.217,0.233,0.239,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
